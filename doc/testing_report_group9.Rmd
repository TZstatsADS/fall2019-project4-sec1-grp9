---
title: "Project4_Group9"
output:
  html_document:
    df_print: paged
---

In this project, we are going to explore matrix factorization methods for recommender system. The goal is to match consumers with most appropriate products. Matrix factorization methods characterize both items and users by vectors of factors inferred from item rating patterns. High correspondence between item and user factors leads to a recommendation. Matrix factorization generally has 3 parts:

- Factorization algorithm

- Regularization

- Postpocessing

We are going to compare P2 vs P3 given A3 + R3.  

***

### Step 1 Load Data and Train-test Split
```{r,message=FALSE}

library(dplyr)
library(tidyr)
library(ggplot2)
library(foreach)
library(doParallel)
library(zoo)
library(Matrix)
library(lubridate)

source("../lib/Matrix_Factorization.R")
source("../lib/cross_validation.R")

set.seed(123)

data <- read.csv("../data/ml-latest-small/ratings.csv")
dataSplit <- train_test_split(data,0.8) #this function ensures that all users and movies are in the training set.
data_train <- dataSplit$train
data_test <- dataSplit$test

```

***
### Step 2 Matrix Factorization

<br/>

#### Step 2.1 Algorithm and Regularization
Here we perform **Alternating Least Squares** to do matrix factorization and use **Temporal Dynamics** as our regularization term.

- For algorithms, the referenced paper are:

A3. [Alternating Least Squares](./paper/P4 Large-scale Parallel Collaborative Filtering for the Netflix Prize.pdf) Section 3.1

R3. [Temporal Dynamics](./paper/P5 Collaborative Filtering with Temporal Dynamics.pdf) Section 4 linear model

<br/>
**Model Setup:**
Let 
$$\hat{r_{ui}} = \mu + b_{i}(t) + b_{u}(t) + q^{T}p_{u}(t)$$
where
$$b_{i}(t) = b_{i} + b_{i,Bin(t)}$$
$$b_{u}(t) = b_{u} + \alpha_{u}dev_{u}(t)$$
$$p_{u}(t) = p_{u} + \alpha_{p}dev_{u}(t)$$
$$dev_{u}(t) = sign(t - t_{u}) |t - t_{u}|^{\beta}$$
and $\mu$ is the average of all ratings, $t_{u}$ is the average rating time of user u, $\beta$ is 0.4 from P5.

The objective function is then: 

$$Minimze \quad \sum_{n=1}^{10} (r_{ui} - \mu - b_{i}- b_{i,Bin(t)} - b_{u} - \alpha_{u}dev_{u}(t) -
q^{T}(p_{u} + \alpha_{p}dev_{u}(t))) + \lambda(||q_{i}^2|| + ||p_{u}^2|| + b_{i}^2 + b_{i,Bin(t)}^2 + b_{u}^2 + \alpha_{u}^2 + \alpha_{p}^2)$$ 

The funciton to apply Alternating Least Square with Temporal Dynamics is als.t(), which can be found in the Matrix_Factorization.r file in the lib folder. For each iteration, the function optimzies parameters $b_i,b_{i,Bin(t)},b_u,\alpha_{u},p,\alpha_p,q$, in that order, to minimize the objective function. 
Please refer to the comments in that function for implementation details.

<br/>

#### Step 2.2 Parameter Tuning

Conducting a full Cross-Validation was very time consuming. Given our time constraints, we tested 6 set of parameters only once. Uncomment the commented section of the code if the user wishes to run the testing process.

```{r, message=FALSE}

f_list <- c(10,20)
l_list <- c(0.01,0.05,0.1)
f_l <- expand.grid(f_list, l_list)

# resultList <- list()
# run_time <- system.time(for(i in 1:nrow(f_l)){
#   par <- paste("f = ", f_l[i,1], ", lambda = ", f_l[i,2])
#   cat(par, "\n")
#   
#   current_result  <- als.t(f = f_l[i,1],  lambda = f_l[i,2],max.iter = 10,data = data, train = data_train, test = data_test)
#   resultList[[paste0('f = ',f_l[i,1],' , lambda = ',f_l[i,2])]] <- current_result
#   
#   
# })
# 
# save(resultList, file = "../output/CVResult.RData")

load('../output/CVResult.RData')
resultNms <- as.list(names(resultList))
rmse <- do.call(rbind,lapply(resultNms, FUN = function(nm){
  x <- resultList[[nm]]$RMSE
  rbind(data.frame(train_test = 'Train',par = nm,Iteration = x$Iteration,RMSE = x$Train.Q.Update,check.names = F),
        data.frame(train_test = 'Test',par = nm,Iteration = x$Iteration,RMSE = x$Test.Q.Update,check.names = F)
  )
}))

rmse %>% ggplot(aes(x = Iteration, y = RMSE, col = train_test)) + geom_point() + 
  facet_grid(~par,labeller = label_wrap_gen(width = 16,multi_line = TRUE)) + scale_x_continuous(breaks = c(0,5,10),labels = c('0','5','10'))


```
  
We can see that the parameter set f = 10, lambda = 0.1 gives us the best outcome (this parameter set also happens to be the one choosen in the starter code). Furthermore, running 3-4 iterations is enough to get the best result.

### Fitting the model with the optimized parameters

```{r}
# registerDoParallel(cores = detectCores())# use parallelization for faster procesing
# result <- als.t(f = 10,  lambda = 0.1,max.iter = 3,data = data, train = data_train, test = data_test)
# 
# save(result,file = "../output/ALSResult.RData")
load('../output/ALSResult.RData')
result$RMSE
```

***

### Step 3 Postprocessing
After matrix factorization, postporcessing will be performed to improve accuracy. We use **Postprocessing SVD with KNN (P2)** and **Postprocessing SVD with kernel (P3)** to do postprocessing.

P2:[Postprocessing SVD with KNN](./paper/P2 Improving regularized singular value decomposition for collaborative filtering .pdf) Section 3.5

P3:[Postprocessing SVD with kernel ridge regression](./paper/P2 Improving regularized singular value decomposition for collaborative filtering .pdf) Section 3.6

We write a function pred_rating_svd_knn to perform Postprocessing SVD with KNN, which is in the p2_scd_knn.R file in the lib folder. 

#### P2 - Postprocessing SVD with KNN
*Uncomment the code to run*
```{r}
source(file = "../lib/p2_svd_knn.R")
## calculate similarity matrix 
# q <- result$q
# p2_result_test <- pred_rating_svd_knn(data_train = data_train, data_test = data_test, q)
# pred_test_adj <- p2_result_test['pred_test']
# test_rmse_p2 <- p2_result_test['rmse_test']
# print(test_rmse_p2)
# save(test_rmse_p2, file = "../output/test_rmse_p2.Rdata")

load("../output/test_rmse_p2.Rdata")
print(test_rmse_p2)
```

#### P3 - Postprocessing SVD with kernel ridge regression 
*Uncomment the code to run*
```{r}
# library("pracma")
# norm_vec <- function(x) {return(x/Norm(x))}
# norm_q <- apply(result$q, 2, norm_vec)
# library("listdtr")
# movieID <- colnames(result$q)
# MSE <- list()
# for(i in 1:610){
#   userID = as.character(i)
#   print(userID)
#   movie_train_index <- which(movieID %in% data_train$movieId[which(data_train$userId == userID)])
#   movie_test_index <- which(movieID %in% data_test$movieId[which(data_test$userId == userID)])
#   obj <- krr(t(norm_q[, movie_train_index]), data_train$rating[movie_train_index])
#   xnew <- norm_q[, movie_test_index]
#   ynew <- predict(obj, t(xnew))
#   ytrue <- data_test$rating[which(data_test$userId == userID)]
#   MSE_ <- mean((ynew - ytrue)^ 2)
#   MSE <- append(MSE, MSE_)
# }
# MSE <- mean(unlist(MSE))
# test_rmse_p3 <- mean(MSE)
# print(test_rmse_p3)
# save(test_rmse_p3, file = "../output/test_rmse_p3.Rdata")

load("../output/test_rmse_p3.Rdata")
print(test_rmse_p3)
```

***

### Step 4 Evaluation
Finally, we compare the rmse of P2 and P3 given A3 + R3. The result is shown in the table below.

```{r}
library(knitr)
test_rmse = c(as.numeric(test_rmse_p2), as.numeric(test_rmse_p3))

compare_df <- data.frame(method = c('P2','P3'), test_rmse = test_rmse)

kable(compare_df, caption = 'Comparision of RMSE for P2 vs P3 given A3 + R3')
```

The test_rmse of Postprocessing SVD with KNN (P2) is smaller, so we can conclude given A3 + R3, P2 is better than P3.